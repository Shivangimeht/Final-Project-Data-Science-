##Peer Graded Assignment Milestone.
**Shivangi Mehta**
**Date--11/7/2021**

**Introduction**

This is the project of Swiftkey company who invent digital Keys on Cell Phone. The purpose of this project is to identify the most frequent words used while texting using mobile phone. We are going to analysis this by using Blogs, twitter, News data set provided by Coursera in Data Science Specialization. 

**TASK 1**

**Getting and Cleaning Data**

**Tasks to accomplish**

TASK 1.1## Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

TASK 1.2## Profanity filtering - removing profanity and other words you do not want to predict.

**Criteria**

Here I have followed below Instruction from Coursera.

1.The link lead to an HTML page describing the exploratory analysis of the training data set.

2.summaries of the three files. Word counts, line counts and basic data tables.

3.The data scientist made basic plots, such as histograms to illustrate features of the data.

4.The report written in a brief.

**load All Require packages**

```{r Final Milestone,warning=FALSE,error=TRUE}
library(dplyr)
library(ggplot2)
require(readtext)
library(sqldf)
library(stringi)
library(quanteda)
```
**Load the Data**

```{r,warning=FALSE,error=TRUE}
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileUrl,destfile="./data/Coursera-SwiftKey.zip")
```
**unzipped the file**

```{r,warning=FALSE,error=TRUE}
unzip <- unzip(zipfile="./data/Coursera-SwiftKey.zip",exdir="./data")
unzip
```
**Checking Data Connection by creating Summary**

```{r,warning=FALSE,error=TRUE}
con <- file("./data/final/en_US/en_US.twitter.txt", "r")
twittertxt <- readLines(con,encoding = "UTF-8", skipNul = TRUE )
con1 <- file("./data/final/en_US/en_US.blogs.txt", "r")
blogstxt <- readLines(con1, encoding = "UTF-8", skipNul = TRUE)
con2 <- file("./data/final/en_US/en_US.news.txt", "r")
newstxt <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
```

**Checking number of lines, words and Size of all Three text file**

```{r,warning=FALSE,error=TRUE}
sizeTwitter <- file.info("./data/final/en_US/en_US.twitter.txt")$size / 1024
sizeblogs <- file.info("./data/final/en_US/en_US.blogs.txt")$size / 1024
sizenews <- file.info("./data/final/en_US/en_US.news.txt")$size / 1024
wordsTwitter <- stri_count_words("./data/final/en_US/en_US.twitter.txt")
wordsblogs <- stri_count_words("./data/final/en_US/en_US.blogs.txt")
wordsnews <- stri_count_words("./data/final/en_US/en_US.news.txt")
data.frame(Source = c("Twitter", "News", "Blogs"),
           SizeMB = c(sizeTwitter,sizeblogs,sizenews),
           NoofLines = c(length(twittertxt),length(blogstxt), length(newstxt)),
           NoOfWords = c(sum(wordsTwitter),sum(wordsblogs), sum(wordsnews)))
```


Since this is the large set of data, this is difficult for us to load all of those within limited RAM memory in personal computer. Therefore, I am collecting Sample out of it as below to complete my analysis. 

**Sampling**


```{r,warning=FALSE,error=TRUE}
set.seed(1288)
twitter_sample <- sample(twittertxt, length(twittertxt)*0.01, replace = FALSE)
blogs_sample <- sample(blogstxt, length(blogstxt)*0.01, replace = FALSE)
news_sample <- sample(newstxt, length(newstxt)*0.01, replace = FALSE)
Sample_data <- c(twitter_sample, blogs_sample, news_sample)
```

The new dataset consists of 32593 words.

**TASK 2**

I am removing those Text which I don't want to analyis.

##Clean Data more in Dept.

```{r,warning=FALSE,error=TRUE}

##Creating Corpus.

##From the above step we got our Corpus.Lets clean data by removing punctuation, remove the numbers and the common english stopwords.

library(tm)

corpus <- VCorpus(VectorSource(list(Sample_data)))

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

for (j in seq(corpus)) {
  corpus[[j]] <- gsub("<.*>", " ", corpus[[j]])
  corpus[[j]] <- gsub("\\b[[:alnum:]]{20,}\\b", " ", corpus[[j]], perl=T)
  corpus[[j]] <- gsub("[[:punct:]]", " ", corpus[[j]])
}
corpus <- tm_map(corpus, PlainTextDocument)
```

##We also want to remove Profanity words to do analysis.I am using googlebad words dictionary.

```{r,warning=FALSE,error=TRUE}
url <- "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt"
if(!file.exists("./data")){dir.create("./data")}
download.file(url,destfile="./data/badwords.txt")
badwords <- read.delim("./data/badwords.txt", sep = ":", header = FALSE)
badwords <- badwords[,1]
badwords <- removeWords(badwords[[458]], stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)

```

In the above R chunks we have cleaned data Sample. We are now ready for Creating Corpus. 

We want to determined and experiment with 3 different grams. Unigram, Bigram and Trigram. We will check words usage frequency and then will create mathematical diagram to check the words frequency and compare all of the three text file. 

**Here is the data frame from the Sample data for n-gram after removing all badwords.

```{r, warning=FALSE,error=TRUE}
data_sample_df <- tibble::tibble(line = 1:length(Sample_data), text = Sample_data)
```

**1. Creating and Showing Unigram.

```{r, warning=FALSE,error=TRUE}
##Let create a plot for Unigram.

UnigramFreq <- data_sample_df %>%
    unnest_tokens(unigram, text, token = "ngrams", n = 3) %>%
    separate(unigram, c("word1"), sep = " ", 
             extra = "drop", fill = "right") %>%
    filter(!word1 %in% stop_words$word) %>%
    unite(unigram, word1, sep = " ") %>%
    count(unigram, sort = TRUE)

ggplot(head(UnigramFreq,15), aes(reorder(unigram,n), n)) +   
    geom_bar(stat="identity") + coord_flip() + 
    xlab("Unigrams") + ylab("Frequency") +
    ggtitle("Most frequent unigrams")

```

**2. Plotting Bigram words frequency data**.

```{r, warning=FALSE,error=TRUE}
##Lets Create Bigram Plot.

library(tidyr)
library(tidytext)

bigram_filtered <- data_sample_df %>%
  unnest_tokens(bigram, text, token= "ngrams", n=2) %>%
  separate(bigram, c("word1","word2"), sep=" ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  dplyr::count(word1, word2, sort=TRUE)


###Unite word 1 and word 2.

bigrams_united <- bigram_filtered %>%
  unite(bigram, word1, word2, sep=" ")
bigrams_united

###Remove NA NA Text which is in First column.

bigrams_united <- bigrams_united[-c(1),]

##Create ggplot after changing n to Frequency.

bigrams_united <- rename(bigrams_united, frequency = n)


ggplot(head(bigrams_united, 20),aes(x=reorder(bigram, frequency),y=frequency)) +
  geom_bar(stat = "Identity") + coord_flip() +
  theme(legend.title=element_blank()) +
  xlab("bigram") + ylab("frequency") +
  labs(title = "Top 20 bigrams by Frequency")

```
**3. Plotting Trigram Frequency.**

```{r,warning=FALSE, error=TRUE}
trigram_filtered <- data_sample_df %>%
  unnest_tokens(trigram, text, token= "ngrams", n=3) %>%
  separate(trigram, c("word1","word2","word3"), sep=" ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  dplyr::count(word1, word2, word3, sort=TRUE)

###Unite word1, word2, word3.

trigrams_united <- trigram_filtered %>%
  unite(trigram, word1, word2, word3, sep=" ")
trigrams_united

##Remove NA NA Text.

trigrams_united <- trigrams_united[-c(1),]

##Lets Plot the Trigram frequency.

trigrams_united <- rename(trigrams_united, frequency = n)

ggplot(head(trigrams_united, 20),aes(x=reorder(trigram, frequency),y=frequency)) +
  geom_bar(stat = "Identity") + coord_flip() +
  theme(legend.title=element_blank()) +
  xlab("trigram") + ylab("frequency") +
  labs(title = "Top 20 trigrams by Frequency")

```

**Report from the Data Analysis**

In this project, I have downloaded data from Coursera. To analysis this data I have used tidyr, tidytext, plyr, tm, RTexttools. To remove profanity words, I used bad words dictionary from Google bad words website. After cleaning all data,I created Sample and by using ngrams function and Tokenization. Then I created plot for Unigram, Bigram and Trigram to determined words used Frequency to help SwiftKey company to create digital Key for social Media.
